{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/lzhao/data/tmp/optiver'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "#book_example = pd.read_parquet(os.path.join(data_dir, 'book_train.parquet/stock_id=0'))\n",
    "#trade_example = pd.read_parquet(os.path.join(data_dir, 'trade_train.parquet/stock_id=0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['row_id'] = train_df['stock_id'].astype(\n",
    "    str) + '-' + train_df['time_id'].astype(str)\n",
    "test_df['row_id'] = test_df['stock_id'].astype(\n",
    "    str) + '-' + test_df['time_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target row_id\n",
       "0         0        5  0.004136    0-5\n",
       "1         0       11  0.001445   0-11\n",
       "2         0       16  0.002168   0-16\n",
       "3         0       31  0.002195   0-31\n",
       "4         0       62  0.001747   0-62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.stock_id.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    428932.000000\n",
       "mean      16038.972721\n",
       "std        9365.103706\n",
       "min           5.000000\n",
       "25%        7854.000000\n",
       "50%       15853.000000\n",
       "75%       23994.000000\n",
       "max       32767.000000\n",
       "Name: time_id, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.time_id.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
      " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
      " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
      " 4]\n"
     ]
    }
   ],
   "source": [
    "def get_cluster_labels(train_p):\n",
    "    train_p = train_p.pivot(\n",
    "        index='time_id', columns='stock_id', values='target')\n",
    "    corr = train_p.corr()\n",
    "    ids = corr.index\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "    print(kmeans.labels_)\n",
    "    l = []\n",
    "    for n in range(7):\n",
    "        l.append([(x-1) for x in ((ids+1)*(kmeans.labels_ == n)) if x > 0])\n",
    "    return l\n",
    "\n",
    "\n",
    "cluster_labels = get_cluster_labels(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算对数收益率\n",
    "def log_return(list_stock_prices):\n",
    "    '''log(s2/s1) = log(s2) - log(s1)'''\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "# 计算已实现波动率\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "# 计算wap价格\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1']\n",
    "           * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2']\n",
    "           * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "\n",
    "    # Calculate spread\n",
    "    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / \\\n",
    "        ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / \\\n",
    "        ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + \\\n",
    "        (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs(\n",
    "        (df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread1': [np.sum, np.mean, np.std],\n",
    "        'price_spread2': [np.sum, np.mean, np.std],\n",
    "        'bid_spread': [np.sum, np.mean, np.std],\n",
    "        'ask_spread': [np.sum, np.mean, np.std],\n",
    "        'total_volume': [np.sum, np.mean, np.std],\n",
    "        'volume_imbalance': [np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\": [np.sum, np.mean, np.std],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(\n",
    "            ['time_id']).agg(create_feature_dict).reset_index()\n",
    "\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300',\n",
    "                    'time_id__200'], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(\n",
    "        lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis=1, inplace=True)\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_trade(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id').price.apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return': [realized_volatility],\n",
    "        'seconds_in_bucket': [count_unique],\n",
    "        'size': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count': [np.mean, np.sum, np.max],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(\n",
    "            ['time_id']).agg(create_feature_dict).reset_index()\n",
    "\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]\n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)\n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(\n",
    "            np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values, 75) - \\\n",
    "            np.percentile(df_id['price'].values, 25)\n",
    "        abs_diff_v = np.median(\n",
    "            np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(\n",
    "            df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n",
    "\n",
    "        lis.append({'time_id': n_time_id, 'tendency': tendencyV, 'f_max': f_max, 'f_min': f_min, 'df_max': df_max, 'df_min': df_min,\n",
    "                   'abs_diff': abs_diff, 'energy': energy, 'iqr_p': iqr_p, 'abs_diff_v': abs_diff_v, 'energy_v': energy_v, 'iqr_p_v': iqr_p_v})\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(\n",
    "        df_lr, how='left', left_on='time_id_', right_on='time_id')\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300',\n",
    "                    'time_id__200', 'time_id'], axis=1, inplace=True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(\n",
    "        lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_stock(df):\n",
    "    # Function to get group stats for the stock_id and time_id\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(\n",
    "        ['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(\n",
    "        ['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how='left', left_on=[\n",
    "                  'stock_id'], right_on=['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how='left', left_on=[\n",
    "                  'time_id'], right_on=['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(list_stock_ids, is_train=True):\n",
    "    # Funtion to make preprocessing function in parallel (for each stock id)\n",
    "\n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = os.path.join(\n",
    "                data_dir, \"book_train.parquet/stock_id=\" + str(stock_id))\n",
    "            file_path_trade = os.path.join(\n",
    "                data_dir, \"trade_train.parquet/stock_id=\" + str(stock_id))\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = os.path.join(\n",
    "                data_dir, \"book_test.parquet/stock_id=\" + str(stock_id))\n",
    "            file_path_trade = os.path.join(\n",
    "                data_dir, \"trade_test.parquet/stock_id=\" + str(stock_id))\n",
    "\n",
    "        # Preprocess book and trade data and merge them\n",
    "        #df_tmp = preprocessor_book(file_path_book)\n",
    "        df_tmp = pd.merge(preprocessor_book(file_path_book), preprocessor_trade(\n",
    "            file_path_trade), on='row_id', how='left')\n",
    "\n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "\n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id)\n",
    "                                        for stock_id in list_stock_ids)\n",
    "\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "train_stock_ids = train_df['stock_id'].unique()\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.merge(train_, on=['row_id'], how='left')\n",
    "train = get_time_stock(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 255 stock_id\n",
      "[-0.0002, -0.0002, -0.0002, -0.0002, 0.00012, 0.00012, 0.00011, 0.00011, -0.0764, -0.04995, -0.03724, -0.02469, 0.0004, 0.00039, 0.00039, 0.00039, 0.00017, 0.00016, 0.00016, 0.00016, 0.15191, 0.09935, 0.07406, 0.04911, 0.0002, 0.0002, 0.0002, 0.00019, 0.00012, 0.00011, 0.00011, 0.00011, 0.07551, 0.0494, 0.03682, 0.02442, -0.0, 0.0, 0.0, 0.0, 0.00423, 0.00334, 0.01001, 0.00334, 0.00109, 0.00153, 0.00285, 0.0087, 0.00285, 0.00091, 0.00133, 0.0023, 0.0072, 0.0023, 0.0007, 0.0011, 0.01244, 0.00423, 0.00139, 0.0019, 0.00023, 0.00022, 0.00022, 0.00022, -1e-05, 0.0, 1e-05, 1e-05, -0.0, 0.0, 0.0, 0.0, 0.00581, 0.00458, 0.01428, 0.00458, 0.00123, 0.00226, 0.00392, 0.01241, 0.00392, 0.00102, 0.00196, 0.00315, 0.01023, 0.00315, 0.00077, 0.00162, 0.01766, 0.00581, 0.00158, 0.00281, 0.00032, 0.00031, 0.0003, 0.0003, -1e-05, 0.0, 1e-05, 1e-05, 0.00066, 0.00065, 0.00064, 0.00064, 0.00022, 0.0002, 0.0002, 0.00019, 0.22264, 0.14366, 0.10655, 0.07036, 0.00106, 0.00104, 0.00103, 0.00103, 0.00027, 0.00024, 0.00024, 0.00023, 0.37455, 0.24301, 0.18061, 0.11947, 3420.4344, 3443.78657, 3451.69438, 3461.53529, 761.88017, 712.29607, 676.88921, 624.77753, 1629942.62415, 1090854.1173, 820102.07391, 547846.35632, 0.00084, 229.68562, 37.15844, 37.22349, 1.00001, 87232840.94517, 44.78805, 44.69044, 0.00164, 324.63028, 0.00267, 0.00214, 0.00636, 0.00214, 0.00079, 0.00086, 0.00183, 0.0055, 0.00183, 0.00062, 0.00075, 0.00147, 0.00454, 0.00147, 0.0004, 0.00064, 0.00788, 0.00267, 0.00106, 0.00104, 26.24763, 21.97894, 19.52483, 16.59972, 3.77498, 3.75583, 3.75104, 3.75154, 373.45339, 243.93439, 181.94748, 121.00316, 89.48841, 59.07306, 44.19934, 29.44312, 3035.39322, 2476.85991, 2166.08142, 1783.58985, 6.39018, 10.99135, 14.43647, 22.88235, 320.67837, 319.72988, 317.94965, 316.15304, 4875.05617, 3832.60739, 3245.50049, 2571.67808, 464.18894, 439.72988, 423.36609, 397.41179, 31861.6255, 20718.76644, 15438.81157, 10260.26487, -17.18966, 840.28531, 844.81243, 844.75531, 844.44321, 681.56895, 647.63599, 621.12156, 581.15392, 396575.44144, 264978.09193, 198661.74001, 132286.9087, 1.0, 1.0, 1.0, 1.00001, 0.00111, 0.00089, 0.00077, 0.00064, 389.92792, 257.64269, 192.72745, 128.18423, 1.0, 1.0, 1.0, 1.0, 0.00115, 0.00094, 0.00082, 0.00069, 389.92767, 257.64254, 192.72735, 128.18416, 0.00028, 0.00027, 0.00027, 0.00027, 0.00022, 0.00021, 0.0002, 0.0002, 0.09234, 0.05945, 0.04407, 0.0291]\n"
     ]
    }
   ],
   "source": [
    "num_col = train.columns.difference(\n",
    "    ['time_id', 'target', 'row_id', 'stock_id']).tolist()\n",
    "num_col = [col for col in num_col if '_stock' not in col]\n",
    "cat_col = 'stock_id'\n",
    "fea_col = [cat_col]+num_col\n",
    "print(len(fea_col), len(num_col), cat_col)\n",
    "print(np.nanmean(train[num_col], axis=0).round(5).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book_data_by_stock_id(stock_id, is_train=True):\n",
    "    if is_train == True:\n",
    "        df = pd.read_parquet(os.path.join(\n",
    "            data_dir, \"book_train.parquet/stock_id=\" + str(stock_id)))\n",
    "    else:\n",
    "        df = pd.read_parquet(os.path.join(\n",
    "            data_dir, \"book_test.parquet/stock_id=\" + str(stock_id)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_price(df):\n",
    "    diff = abs(\n",
    "        df[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']].diff())\n",
    "    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n",
    "    n_ticks = (diff / min_diff).round()\n",
    "    scale = 0.01 / np.nanmean(diff / n_ticks)\n",
    "    return scale\n",
    "\n",
    "\n",
    "def calc_prices(stock_id, is_train):\n",
    "    try:\n",
    "        book = load_book_data_by_stock_id(stock_id, is_train)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    df = book.groupby('time_id').apply(\n",
    "        calc_price).to_frame('price').reset_index()\n",
    "    df['stock_id'] = stock_id\n",
    "    df['first_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].first().values\n",
    "    df['last_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].last().values\n",
    "    df['mean_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].mean().values\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1858s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0310s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0430s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0509s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 108 out of 112 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  35 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  37 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  39 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  51 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  54 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  55 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  57 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  59 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  61 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  63 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  65 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  66 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  67 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  70 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  71 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  73 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=4)]: Done  74 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done  75 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done  78 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  79 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  80 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  81 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done  82 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  83 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  85 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done  86 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  87 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  89 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  91 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=4)]: Done  94 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=4)]: Done  95 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  96 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  97 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done  98 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done  99 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done 100 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done 108 out of 112 | elapsed:  5.4min remaining:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:  5.6min finished\n"
     ]
    }
   ],
   "source": [
    "prices_df1 = pd.concat(Parallel(n_jobs=4, verbose=51)(\n",
    "    delayed(calc_prices)(r, False) for r in train_stock_ids))\n",
    "prices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(\n",
    "    delayed(calc_prices)(r, True) for r in train_stock_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = pd.concat([prices_df1, prices_df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428933, 6)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot(\n",
    "    'time_id', 'stock_id', 'first_wap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot(\n",
    "    'time_id', 'stock_id', 'last_wap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = first_df.fillna(first_df.mean())\n",
    "last_df = last_df.fillna(first_df.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearby_time_id(query_df, template_df):\n",
    "    def get_nearby_time_id_(time_id):\n",
    "        template_df_ = template_df.drop(time_id, axis=1)\n",
    "        query = query_df[time_id].values.repeat(\n",
    "            len(template_df_.columns)).reshape(-1, len(template_df_.columns))\n",
    "        diffs = np.square((template_df_ - query)/query).sum(axis=0)\n",
    "        diffs = diffs.sort_values()[:2].reset_index().rename(\n",
    "            columns={'time_id': 'tid', 0: 'loss'})\n",
    "        diffs['time_id'] = time_id\n",
    "        return diffs\n",
    "    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n",
    "    edge_df = edge_df[['time_id', 'tid', 'loss']]\n",
    "    edge_df.columns = ['a', 'b', 'w']\n",
    "    return edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e1f820d85e4eb88349cfb0bb302671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 116 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 1316 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done 3316 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=4)]: Done 3831 out of 3831 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554e684b44ba45779e2e245deb4525c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 200 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 1400 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=4)]: Done 3400 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=4)]: Done 3831 out of 3831 | elapsed:   18.9s finished\n"
     ]
    }
   ],
   "source": [
    "last_edge_df = get_nearby_time_id(last_df.T, first_df.T)\n",
    "first_edge_df = get_nearby_time_id(first_df.T, last_df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.concat([first_edge_df, last_edge_df])\n",
    "edge_df = edge_df.groupby(['a', 'b'])['w'].min().to_frame('w').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1205</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>26708</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>30183</td>\n",
       "      <td>0.000688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>29583</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a      b         w\n",
       "0   5   1205  0.001942\n",
       "1   5  26708  0.001290\n",
       "2   5  30183  0.000688\n",
       "3  11   2811  0.000387\n",
       "4  11  29583  0.000537"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10119, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRD = 0.02\n",
    "edge_df = edge_df.query('w < @THRD').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9768, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''拓展到所有stock'''\n",
    "edge_dfs = []\n",
    "for stock_id in train_stock_ids:\n",
    "    edge_df_ = edge_df.copy()\n",
    "    edge_df_['a'] = str(stock_id)+'-' + edge_df.a.astype(str)\n",
    "    edge_df_['b'] = str(stock_id)+'-' + edge_df.b.astype(str)\n",
    "    edge_dfs.append(edge_df_)\n",
    "edge_df = pd.concat(edge_dfs).reset_index(drop=True)\n",
    "# edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094016, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
