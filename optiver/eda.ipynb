{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/lzhao/data/tmp/optiver'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "#book_example = pd.read_parquet(os.path.join(data_dir, 'book_train.parquet/stock_id=0'))\n",
    "#trade_example = pd.read_parquet(os.path.join(data_dir, 'trade_train.parquet/stock_id=0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['row_id'] = train_df['stock_id'].astype(\n",
    "    str) + '-' + train_df['time_id'].astype(str)\n",
    "test_df['row_id'] = test_df['stock_id'].astype(\n",
    "    str) + '-' + test_df['time_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target row_id\n",
       "0         0        5  0.004136    0-5\n",
       "1         0       11  0.001445   0-11\n",
       "2         0       16  0.002168   0-16\n",
       "3         0       31  0.002195   0-31\n",
       "4         0       62  0.001747   0-62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.stock_id.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    428932.000000\n",
       "mean      16038.972721\n",
       "std        9365.103706\n",
       "min           5.000000\n",
       "25%        7854.000000\n",
       "50%       15853.000000\n",
       "75%       23994.000000\n",
       "max       32767.000000\n",
       "Name: time_id, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.time_id.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
      " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
      " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
      " 4]\n"
     ]
    }
   ],
   "source": [
    "def get_cluster_labels(train_p):\n",
    "    train_p = train_p.pivot(\n",
    "        index='time_id', columns='stock_id', values='target')\n",
    "    corr = train_p.corr()\n",
    "    ids = corr.index\n",
    "    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "    print(kmeans.labels_)\n",
    "    l = []\n",
    "    for n in range(7):\n",
    "        l.append([(x-1) for x in ((ids+1)*(kmeans.labels_ == n)) if x > 0])\n",
    "    return l\n",
    "\n",
    "\n",
    "cluster_labels = get_cluster_labels(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算对数收益率\n",
    "def log_return(list_stock_prices):\n",
    "    '''log(s2/s1) = log(s2) - log(s1)'''\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "# 计算已实现波动率\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "# 计算wap价格\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1']\n",
    "           * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2']\n",
    "           * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "\n",
    "    # Calculate spread\n",
    "    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / \\\n",
    "        ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / \\\n",
    "        ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + \\\n",
    "        (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs(\n",
    "        (df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread1': [np.sum, np.mean, np.std],\n",
    "        'price_spread2': [np.sum, np.mean, np.std],\n",
    "        'bid_spread': [np.sum, np.mean, np.std],\n",
    "        'ask_spread': [np.sum, np.mean, np.std],\n",
    "        'total_volume': [np.sum, np.mean, np.std],\n",
    "        'volume_imbalance': [np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\": [np.sum, np.mean, np.std],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(\n",
    "            ['time_id']).agg(create_feature_dict).reset_index()\n",
    "\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300',\n",
    "                    'time_id__200'], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(\n",
    "        lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis=1, inplace=True)\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_trade(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id').price.apply(log_return)\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return': [realized_volatility],\n",
    "        'seconds_in_bucket': [count_unique],\n",
    "        'size': [np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count': [np.mean, np.sum, np.max],\n",
    "    }\n",
    "\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix=False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(\n",
    "            ['time_id']).agg(create_feature_dict).reset_index()\n",
    "\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket=400, add_suffix=True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket=300, add_suffix=True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket=200, add_suffix=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]\n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)\n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max = np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min = np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(\n",
    "            np.abs(df_id['price'].values - np.mean(df_id['price'].values)))\n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values, 75) - \\\n",
    "            np.percentile(df_id['price'].values, 25)\n",
    "        abs_diff_v = np.median(\n",
    "            np.abs(df_id['size'].values - np.mean(df_id['size'].values)))\n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(\n",
    "            df_id['size'].values, 75) - np.percentile(df_id['size'].values, 25)\n",
    "\n",
    "        lis.append({'time_id': n_time_id, 'tendency': tendencyV, 'f_max': f_max, 'f_min': f_min, 'df_max': df_max, 'df_min': df_min,\n",
    "                   'abs_diff': abs_diff, 'energy': energy, 'iqr_p': iqr_p, 'abs_diff_v': abs_diff_v, 'energy_v': energy_v, 'iqr_p_v': iqr_p_v})\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(\n",
    "        df_lr, how='left', left_on='time_id_', right_on='time_id')\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_400, how='left', left_on='time_id_', right_on='time_id__400')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_300, how='left', left_on='time_id_', right_on='time_id__300')\n",
    "    df_feature = df_feature.merge(\n",
    "        df_feature_200, how='left', left_on='time_id_', right_on='time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300',\n",
    "                    'time_id__200', 'time_id'], axis=1, inplace=True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(\n",
    "        lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_stock(df):\n",
    "    # Function to get group stats for the stock_id and time_id\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(\n",
    "        ['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(\n",
    "        ['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how='left', left_on=[\n",
    "                  'stock_id'], right_on=['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how='left', left_on=[\n",
    "                  'time_id'], right_on=['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(list_stock_ids, is_train=True):\n",
    "    # Funtion to make preprocessing function in parallel (for each stock id)\n",
    "\n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = os.path.join(\n",
    "                data_dir, \"book_train.parquet/stock_id=\" + str(stock_id))\n",
    "            file_path_trade = os.path.join(\n",
    "                data_dir, \"trade_train.parquet/stock_id=\" + str(stock_id))\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = os.path.join(\n",
    "                data_dir, \"book_test.parquet/stock_id=\" + str(stock_id))\n",
    "            file_path_trade = os.path.join(\n",
    "                data_dir, \"trade_test.parquet/stock_id=\" + str(stock_id))\n",
    "\n",
    "        # Preprocess book and trade data and merge them\n",
    "        #df_tmp = preprocessor_book(file_path_book)\n",
    "        df_tmp = pd.merge(preprocessor_book(file_path_book), preprocessor_trade(\n",
    "            file_path_trade), on='row_id', how='left')\n",
    "\n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "\n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id)\n",
    "                                        for stock_id in list_stock_ids)\n",
    "\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "train_stock_ids = train_df['stock_id'].unique()\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "test_stock_ids = test_df['stock_id'].unique()\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.merge(train_, on=['row_id'], how='left')\n",
    "train = get_time_stock(train)\n",
    "\n",
    "test = test_df.merge(test_, on=['row_id'], how='left')\n",
    "test = get_time_stock(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 255 stock_id\n",
      "[-0.0002, -0.0002, -0.0002, -0.0002, 0.00012, 0.00012, 0.00011, 0.00011, -0.0764, -0.04995, -0.03724, -0.02469, 0.0004, 0.00039, 0.00039, 0.00039, 0.00017, 0.00016, 0.00016, 0.00016, 0.15191, 0.09935, 0.07406, 0.04911, 0.0002, 0.0002, 0.0002, 0.00019, 0.00012, 0.00011, 0.00011, 0.00011, 0.07551, 0.0494, 0.03682, 0.02442, -0.0, 0.0, 0.0, 0.0, 0.00423, 0.00334, 0.01001, 0.00334, 0.00109, 0.00153, 0.00285, 0.0087, 0.00285, 0.00091, 0.00133, 0.0023, 0.0072, 0.0023, 0.0007, 0.0011, 0.01244, 0.00423, 0.00139, 0.0019, 0.00023, 0.00022, 0.00022, 0.00022, -1e-05, 0.0, 1e-05, 1e-05, -0.0, 0.0, 0.0, 0.0, 0.00581, 0.00458, 0.01428, 0.00458, 0.00123, 0.00226, 0.00392, 0.01241, 0.00392, 0.00102, 0.00196, 0.00315, 0.01023, 0.00315, 0.00077, 0.00162, 0.01766, 0.00581, 0.00158, 0.00281, 0.00032, 0.00031, 0.0003, 0.0003, -1e-05, 0.0, 1e-05, 1e-05, 0.00066, 0.00065, 0.00064, 0.00064, 0.00022, 0.0002, 0.0002, 0.00019, 0.22264, 0.14366, 0.10655, 0.07036, 0.00106, 0.00104, 0.00103, 0.00103, 0.00027, 0.00024, 0.00024, 0.00023, 0.37455, 0.24301, 0.18061, 0.11947, 3420.4344, 3443.78657, 3451.69438, 3461.53529, 761.88017, 712.29607, 676.88921, 624.77753, 1629942.62415, 1090854.1173, 820102.07391, 547846.35632, 0.00084, 229.68562, 37.15844, 37.22349, 1.00001, 87232840.94517, 44.78805, 44.69044, 0.00164, 324.63028, 0.00267, 0.00214, 0.00636, 0.00214, 0.00079, 0.00086, 0.00183, 0.0055, 0.00183, 0.00062, 0.00075, 0.00147, 0.00454, 0.00147, 0.0004, 0.00064, 0.00788, 0.00267, 0.00106, 0.00104, 26.24763, 21.97894, 19.52483, 16.59972, 3.77498, 3.75583, 3.75104, 3.75154, 373.45339, 243.93439, 181.94748, 121.00316, 89.48841, 59.07306, 44.19934, 29.44312, 3035.39322, 2476.85991, 2166.08142, 1783.58985, 6.39018, 10.99135, 14.43647, 22.88235, 320.67837, 319.72988, 317.94965, 316.15304, 4875.05617, 3832.60739, 3245.50049, 2571.67808, 464.18894, 439.72988, 423.36609, 397.41179, 31861.6255, 20718.76644, 15438.81157, 10260.26487, -17.18966, 840.28531, 844.81243, 844.75531, 844.44321, 681.56895, 647.63599, 621.12156, 581.15392, 396575.44144, 264978.09193, 198661.74001, 132286.9087, 1.0, 1.0, 1.0, 1.00001, 0.00111, 0.00089, 0.00077, 0.00064, 389.92792, 257.64269, 192.72745, 128.18423, 1.0, 1.0, 1.0, 1.0, 0.00115, 0.00094, 0.00082, 0.00069, 389.92767, 257.64254, 192.72735, 128.18416, 0.00028, 0.00027, 0.00027, 0.00027, 0.00022, 0.00021, 0.0002, 0.0002, 0.09234, 0.05945, 0.04407, 0.0291]\n"
     ]
    }
   ],
   "source": [
    "num_col = train.columns.difference(\n",
    "    ['time_id', 'target', 'row_id', 'stock_id', 'price']).tolist()\n",
    "num_col = [col for col in num_col if '_stock' not in col]\n",
    "cat_col = 'stock_id'\n",
    "fea_col = [cat_col]+num_col\n",
    "print(len(fea_col), len(num_col), cat_col)\n",
    "print(np.nanmean(train[num_col], axis=0).round(5).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[num_col] = train[num_col].fillna(train[num_col].mean())\n",
    "test[num_col] = test[num_col].fillna(test[num_col].mean())\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "gg = 50\n",
    "for i in range(len(num_col)//gg+1):\n",
    "    col = num_col[i*gg:i*gg+gg]\n",
    "    train[col] = scaler.fit_transform(train[col])\n",
    "    test[col] = scaler.transform(test[col])\n",
    "\n",
    "node_df = pd.concat([train,test]).drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_log_return_realized_volatility_400_max_time</th>\n",
       "      <th>trade_log_return_realized_volatility_400_min_time</th>\n",
       "      <th>trade_log_return_realized_volatility_300_mean_time</th>\n",
       "      <th>trade_log_return_realized_volatility_300_std_time</th>\n",
       "      <th>trade_log_return_realized_volatility_300_max_time</th>\n",
       "      <th>trade_log_return_realized_volatility_300_min_time</th>\n",
       "      <th>trade_log_return_realized_volatility_200_mean_time</th>\n",
       "      <th>trade_log_return_realized_volatility_200_std_time</th>\n",
       "      <th>trade_log_return_realized_volatility_200_max_time</th>\n",
       "      <th>trade_log_return_realized_volatility_200_min_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "      <td>-0.085455</td>\n",
       "      <td>0.066727</td>\n",
       "      <td>-0.960597</td>\n",
       "      <td>-0.085531</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>-0.954658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788012</td>\n",
       "      <td>-0.741007</td>\n",
       "      <td>-0.819421</td>\n",
       "      <td>-0.784986</td>\n",
       "      <td>-0.787718</td>\n",
       "      <td>-0.807336</td>\n",
       "      <td>-0.815416</td>\n",
       "      <td>-0.745864</td>\n",
       "      <td>-0.795600</td>\n",
       "      <td>-0.786195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "      <td>-0.423188</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>-0.985695</td>\n",
       "      <td>-0.423215</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>-0.984241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.882373</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.952860</td>\n",
       "      <td>-0.892822</td>\n",
       "      <td>-0.904850</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.950247</td>\n",
       "      <td>-0.872260</td>\n",
       "      <td>-0.937439</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "      <td>-0.462945</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.950623</td>\n",
       "      <td>-0.462864</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>-0.949930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872752</td>\n",
       "      <td>-0.888950</td>\n",
       "      <td>-0.924542</td>\n",
       "      <td>-0.907666</td>\n",
       "      <td>-0.885537</td>\n",
       "      <td>-0.865293</td>\n",
       "      <td>-0.925755</td>\n",
       "      <td>-0.905138</td>\n",
       "      <td>-0.905782</td>\n",
       "      <td>-0.819135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "      <td>-0.685924</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>-0.956893</td>\n",
       "      <td>-0.686003</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.961920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.817992</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.931598</td>\n",
       "      <td>-0.827622</td>\n",
       "      <td>-0.812109</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.927312</td>\n",
       "      <td>-0.829207</td>\n",
       "      <td>-0.846194</td>\n",
       "      <td>-0.932606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "      <td>-0.502200</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>-0.985938</td>\n",
       "      <td>-0.502200</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>-0.981666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.925554</td>\n",
       "      <td>-0.893352</td>\n",
       "      <td>-0.966579</td>\n",
       "      <td>-0.933911</td>\n",
       "      <td>-0.925097</td>\n",
       "      <td>-0.924525</td>\n",
       "      <td>-0.963864</td>\n",
       "      <td>-0.921534</td>\n",
       "      <td>-0.920973</td>\n",
       "      <td>-0.920861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target row_id  wap1_sum  wap1_mean  wap1_std  \\\n",
       "0         0        5  0.004136    0-5 -0.085455   0.066727 -0.960597   \n",
       "1         0       11  0.001445   0-11 -0.423188   0.023678 -0.985695   \n",
       "2         0       16  0.002168   0-16 -0.462945   0.015071 -0.950623   \n",
       "3         0       31  0.002195   0-31 -0.685924   0.006301 -0.956893   \n",
       "4         0       62  0.001747   0-62 -0.502200   0.016019 -0.985938   \n",
       "\n",
       "   wap2_sum  wap2_mean  wap2_std  ...  \\\n",
       "0 -0.085531   0.065502 -0.954658  ...   \n",
       "1 -0.423215   0.022876 -0.984241  ...   \n",
       "2 -0.462864   0.016387 -0.949930  ...   \n",
       "3 -0.686003   0.003469 -0.961920  ...   \n",
       "4 -0.502200   0.015728 -0.981666  ...   \n",
       "\n",
       "   trade_log_return_realized_volatility_400_max_time  \\\n",
       "0                                          -0.788012   \n",
       "1                                          -0.882373   \n",
       "2                                          -0.872752   \n",
       "3                                          -0.817992   \n",
       "4                                          -0.925554   \n",
       "\n",
       "   trade_log_return_realized_volatility_400_min_time  \\\n",
       "0                                          -0.741007   \n",
       "1                                          -1.000000   \n",
       "2                                          -0.888950   \n",
       "3                                          -1.000000   \n",
       "4                                          -0.893352   \n",
       "\n",
       "   trade_log_return_realized_volatility_300_mean_time  \\\n",
       "0                                          -0.819421    \n",
       "1                                          -0.952860    \n",
       "2                                          -0.924542    \n",
       "3                                          -0.931598    \n",
       "4                                          -0.966579    \n",
       "\n",
       "   trade_log_return_realized_volatility_300_std_time  \\\n",
       "0                                          -0.784986   \n",
       "1                                          -0.892822   \n",
       "2                                          -0.907666   \n",
       "3                                          -0.827622   \n",
       "4                                          -0.933911   \n",
       "\n",
       "   trade_log_return_realized_volatility_300_max_time  \\\n",
       "0                                          -0.787718   \n",
       "1                                          -0.904850   \n",
       "2                                          -0.885537   \n",
       "3                                          -0.812109   \n",
       "4                                          -0.925097   \n",
       "\n",
       "   trade_log_return_realized_volatility_300_min_time  \\\n",
       "0                                          -0.807336   \n",
       "1                                          -1.000000   \n",
       "2                                          -0.865293   \n",
       "3                                          -1.000000   \n",
       "4                                          -0.924525   \n",
       "\n",
       "   trade_log_return_realized_volatility_200_mean_time  \\\n",
       "0                                          -0.815416    \n",
       "1                                          -0.950247    \n",
       "2                                          -0.925755    \n",
       "3                                          -0.927312    \n",
       "4                                          -0.963864    \n",
       "\n",
       "   trade_log_return_realized_volatility_200_std_time  \\\n",
       "0                                          -0.745864   \n",
       "1                                          -0.872260   \n",
       "2                                          -0.905138   \n",
       "3                                          -0.829207   \n",
       "4                                          -0.921534   \n",
       "\n",
       "   trade_log_return_realized_volatility_200_max_time  \\\n",
       "0                                          -0.795600   \n",
       "1                                          -0.937439   \n",
       "2                                          -0.905782   \n",
       "3                                          -0.846194   \n",
       "4                                          -0.920973   \n",
       "\n",
       "   trade_log_return_realized_volatility_200_min_time  \n",
       "0                                          -0.786195  \n",
       "1                                          -1.000000  \n",
       "2                                          -0.819135  \n",
       "3                                          -0.932606  \n",
       "4                                          -0.920861  \n",
       "\n",
       "[5 rows x 307 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book_data_by_stock_id(stock_id, is_train=True):\n",
    "    if is_train == True:\n",
    "        df = pd.read_parquet(os.path.join(\n",
    "            data_dir, \"book_train.parquet/stock_id=\" + str(stock_id)))\n",
    "    else:\n",
    "        df = pd.read_parquet(os.path.join(\n",
    "            data_dir, \"book_test.parquet/stock_id=\" + str(stock_id)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_price(df):\n",
    "    diff = abs(\n",
    "        df[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']].diff())\n",
    "    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n",
    "    n_ticks = (diff / min_diff).round()\n",
    "    scale = 0.01 / np.nanmean(diff / n_ticks)\n",
    "    return scale\n",
    "\n",
    "\n",
    "def calc_prices(stock_id, is_train):\n",
    "    try:\n",
    "        book = load_book_data_by_stock_id(stock_id, is_train)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    df = book.groupby('time_id').apply(\n",
    "        calc_price).to_frame('price').reset_index()\n",
    "    df['stock_id'] = stock_id\n",
    "    df['first_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].first().values\n",
    "    df['last_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].last().values\n",
    "    df['mean_wap'] = df['price'] * \\\n",
    "        book.groupby('time_id')['wap1'].mean().values\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  35 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  37 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  39 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done  51 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done  54 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done  55 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done  57 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  59 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done  61 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  63 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  65 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  66 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  67 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=4)]: Done  70 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  71 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=4)]: Done  73 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=4)]: Done  74 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done  75 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done  78 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  79 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done  80 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done  81 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done  82 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  83 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done  85 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done  86 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  87 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done  89 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  91 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=4)]: Done  94 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  95 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  96 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  97 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done  98 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done  99 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done 100 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done 108 out of 112 | elapsed:  5.4min remaining:   12.1s\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:  5.6min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0381s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "prices_df1 = pd.concat(Parallel(n_jobs=4, verbose=51)(\n",
    "    delayed(calc_prices)(r, True) for r in train_stock_ids))\n",
    "prices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(\n",
    "    delayed(calc_prices)(r, False) for r in test_stock_ids))\n",
    "\n",
    "prices_df = pd.concat([prices_df1, prices_df2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428933, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot('time_id', 'stock_id', 'first_wap')\n",
    "last_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot('time_id', 'stock_id', 'last_wap')\n",
    "\n",
    "first_df = first_df.fillna(first_df.mean())\n",
    "last_df = last_df.fillna(first_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearby_time_id(query_df, template_df):\n",
    "    def get_nearby_time_id_(time_id):\n",
    "        template_df_ = template_df.drop(time_id, axis=1)\n",
    "        query = query_df[time_id].values.repeat(\n",
    "            len(template_df_.columns)).reshape(-1, len(template_df_.columns))\n",
    "        diffs = np.square((template_df_ - query)/query).sum(axis=0)\n",
    "        diffs = diffs.sort_values()[:2].reset_index().rename(\n",
    "            columns={'time_id': 'tid', 0: 'loss'})\n",
    "        diffs['time_id'] = time_id\n",
    "        return diffs\n",
    "    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n",
    "    edge_df = edge_df[['time_id', 'tid', 'loss']]\n",
    "    edge_df.columns = ['a', 'b', 'w']\n",
    "    return edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331aeddbf1904c15bff20c4d0df7fca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 1288 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=4)]: Done 3288 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=4)]: Done 3831 out of 3831 | elapsed:   19.4s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eddc5b2b0f44f29415ca69c5f52465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 200 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 1400 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done 3400 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=4)]: Done 3831 out of 3831 | elapsed:   18.4s finished\n"
     ]
    }
   ],
   "source": [
    "last_edge_df = get_nearby_time_id(last_df.T, first_df.T)\n",
    "first_edge_df = get_nearby_time_id(first_df.T, last_df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.concat([first_edge_df, last_edge_df])\n",
    "edge_df = edge_df.groupby(['a','b'])['w'].min().to_frame('w').reset_index().rename(columns={'a':'b','b':'a'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRD = 0.02\n",
    "edge_df = edge_df.query('w < @THRD').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9768, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''拓展到所有stock'''\n",
    "edge_dfs = []\n",
    "for stock_id in train_stock_ids:\n",
    "    edge_df_ = edge_df.copy()\n",
    "    edge_df_['a'] = str(stock_id)+'-' + edge_df.a.astype(str)\n",
    "    edge_df_['b'] = str(stock_id)+'-' + edge_df.b.astype(str)\n",
    "    edge_dfs.append(edge_df_)\n",
    "edge_df = pd.concat(edge_dfs).reset_index(drop=True)\n",
    "# edge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>a</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0-1205</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0-26708</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0-30183</td>\n",
       "      <td>0.000688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0-2811</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0-29583</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      b        a         w\n",
       "0   0-5   0-1205  0.001942\n",
       "1   0-5  0-26708  0.001290\n",
       "2   0-5  0-30183  0.000688\n",
       "3  0-11   0-2811  0.000387\n",
       "4  0-11  0-29583  0.000537"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1112214, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''孤立点'''\n",
    "isolate_node = list(set(node_df['row_id']) - set(edge_df['a'].unique()))\n",
    "isolate_edge_df = pd.DataFrame({'a':isolate_node,'b':isolate_node,'w':0})\n",
    "edge_df = pd.concat([edge_df,isolate_edge_df]).reset_index(drop=True)\n",
    "\n",
    "'''重新编码'''\n",
    "row_id_index_dict = dict(zip(node_df.row_id,node_df.index))\n",
    "\n",
    "edge_df['a'] = edge_df['a'].map(row_id_index_dict)\n",
    "edge_df['b'] = edge_df['b'].map(row_id_index_dict)\n",
    "\n",
    "'''构造边使用了笛卡尔积，但是有些点不存在'''\n",
    "edge_df = edge_df[~edge_df.a.isna() & ~edge_df.b.isna()]\n",
    "edge_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, n_channels: int, out_channels: int, hidden_channels: int = 128, num_layers=3, heads=4):\n",
    "        super.__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        hidden_channels = self.hidden_channels\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(TransformerConv(in_channels, hidden_channels//heads,heads=heads))\n",
    "        self.convs.append(TransformerConv(hidden_channels, hidden_channels//heads,heads=heads))\n",
    "        self.convs.append(TransformerConv(hidden_channels, hidden_channels//heads,heads=heads))\n",
    "        self.embedding = nn.Embedding(127, GS_CFG.stock_embedding_size)\n",
    "        self.Lin1 = nn.Linear(hidden_channels,out_channels)\n",
    "\n",
    "    def forward(self, data):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
